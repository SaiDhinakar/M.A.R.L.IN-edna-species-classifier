{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28d4eafa",
   "metadata": {},
   "source": [
    "# M.A.R.L.IN eDNA Species Classifier - Model Training\n",
    "\n",
    "## Overview\n",
    "This notebook trains deep learning models for:\n",
    "1. **Taxonomic Classification**: Predicting taxonomic labels from sequence embeddings\n",
    "2. **Novelty Detection**: Identifying potentially novel species/taxa\n",
    "3. **Cluster Prediction**: Assigning sequences to biological clusters\n",
    "4. **Database Classification**: Predicting the most likely reference database\n",
    "\n",
    "## Models Architecture\n",
    "- **Feed-forward Neural Networks**: For embedding-based classification\n",
    "- **CNN Models**: For sequence-based learning (optional)\n",
    "- **Ensemble Methods**: Combining multiple models for better predictions\n",
    "- **Autoencoder**: For novelty detection based on reconstruction error\n",
    "\n",
    "## Training Strategy\n",
    "- Stratified train/validation/test splits\n",
    "- Cross-validation for robust evaluation\n",
    "- Hyperparameter optimization\n",
    "- Early stopping and regularization\n",
    "- Class imbalance handling\n",
    "\n",
    "## Goals\n",
    "- Train robust models for taxonomic classification\n",
    "- Develop effective novelty detection system\n",
    "- Create model ensemble for improved accuracy\n",
    "- Generate model confidence scores and uncertainty estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82d3cf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model training directory: ../model\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_curve, auc\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = Path(\"../data\")\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "EMBEDDINGS_DIR = BASE_DIR / \"embeddings\"\n",
    "MODEL_DIR = Path(\"../model\")\n",
    "DNABERT_DIR = MODEL_DIR / \"dnabert_finetuned\"\n",
    "\n",
    "# Create directories\n",
    "DNABERT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Model training directory: {MODEL_DIR}\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b066af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data for model training...\n",
      "Loaded 15000 sequences with embeddings\n",
      "Embedding dimensions: 1344\n",
      "Original embeddings shape: (15000, 1344)\n",
      "PCA embeddings shape: (15000, 50)\n",
      "\n",
      "Target variable distributions:\n",
      "Databases: Counter({'16S_ribosomal_RNA': 5000, '18S_fungal_sequences': 5000, '28S_fungal_sequences': 5000})\n",
      "Clusters (no noise): 1 clusters\n",
      "Novelty: Counter({np.int64(0): 15000}) (0=known, 1=novel)\n",
      "\n",
      "Data preparation:\n",
      "Total sequences: 15000\n",
      "Sequences with clusters (no noise): 15000\n",
      "Novel sequences: 0\n",
      "Known sequences: 15000\n",
      "Loaded 15000 sequences with embeddings\n",
      "Embedding dimensions: 1344\n",
      "Original embeddings shape: (15000, 1344)\n",
      "PCA embeddings shape: (15000, 50)\n",
      "\n",
      "Target variable distributions:\n",
      "Databases: Counter({'16S_ribosomal_RNA': 5000, '18S_fungal_sequences': 5000, '28S_fungal_sequences': 5000})\n",
      "Clusters (no noise): 1 clusters\n",
      "Novelty: Counter({np.int64(0): 15000}) (0=known, 1=novel)\n",
      "\n",
      "Data preparation:\n",
      "Total sequences: 15000\n",
      "Sequences with clusters (no noise): 15000\n",
      "Novel sequences: 0\n",
      "Known sequences: 15000\n"
     ]
    }
   ],
   "source": [
    "# Load data for model training\n",
    "print(\"Loading processed data for model training...\")\n",
    "\n",
    "try:\n",
    "    # Load clustered sequences with embeddings\n",
    "    with open(PROCESSED_DIR / \"sequences_clustered.pkl\", 'rb') as f:\n",
    "        df_sequences = pickle.load(f)\n",
    "    \n",
    "    # Load embeddings\n",
    "    with open(EMBEDDINGS_DIR / \"sequence_embeddings.pkl\", 'rb') as f:\n",
    "        embeddings_data = pickle.load(f)\n",
    "    \n",
    "    # Load biodiversity analysis\n",
    "    with open(PROCESSED_DIR / \"biodiversity_analysis.pkl\", 'rb') as f:\n",
    "        analysis_results = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(df_sequences)} sequences with embeddings\")\n",
    "    print(f\"Embedding dimensions: {embeddings_data['original_embeddings'].shape[1]}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please run all previous notebooks first!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Extract embeddings and prepare features\n",
    "X_original = embeddings_data['original_embeddings']\n",
    "X_pca = embeddings_data['pca_embeddings']\n",
    "\n",
    "print(f\"Original embeddings shape: {X_original.shape}\")\n",
    "print(f\"PCA embeddings shape: {X_pca.shape}\")\n",
    "\n",
    "# Prepare target variables for different tasks\n",
    "# 1. Database classification\n",
    "y_database = df_sequences['database'].values\n",
    "database_encoder = LabelEncoder()\n",
    "y_database_encoded = database_encoder.fit_transform(y_database)\n",
    "\n",
    "# 2. Cluster prediction (remove noise points)\n",
    "mask_no_noise = df_sequences['cluster_id'] != -1\n",
    "X_clustered = X_pca[mask_no_noise]\n",
    "y_cluster = df_sequences[mask_no_noise]['cluster_id'].values\n",
    "cluster_encoder = LabelEncoder()\n",
    "y_cluster_encoded = cluster_encoder.fit_transform(y_cluster)\n",
    "\n",
    "# 3. Novelty detection (binary: novel vs known)\n",
    "novelty_threshold = 0.5\n",
    "y_novelty = (df_sequences['cluster_novelty'] >= novelty_threshold).astype(int).values\n",
    "\n",
    "print(f\"\\nTarget variable distributions:\")\n",
    "print(f\"Databases: {Counter(y_database)}\")\n",
    "print(f\"Clusters (no noise): {len(np.unique(y_cluster_encoded))} clusters\")\n",
    "print(f\"Novelty: {Counter(y_novelty)} (0=known, 1=novel)\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nData preparation:\")\n",
    "print(f\"Total sequences: {len(df_sequences)}\")\n",
    "print(f\"Sequences with clusters (no noise): {len(X_clustered)}\")\n",
    "print(f\"Novel sequences: {np.sum(y_novelty)}\")\n",
    "print(f\"Known sequences: {len(y_novelty) - np.sum(y_novelty)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10f2cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trainer initialized!\n"
     ]
    }
   ],
   "source": [
    "# Define neural network models\n",
    "class TaxonomicClassifier(nn.Module):\n",
    "    \"\"\"Neural network for taxonomic classification\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes, hidden_dims=[256, 128, 64], dropout_rate=0.3):\n",
    "        super(TaxonomicClassifier, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class NoveltyDetector(nn.Module):\n",
    "    \"\"\"Autoencoder for novelty detection\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, encoding_dims=[128, 64, 32], dropout_rate=0.2):\n",
    "        super(NoveltyDetector, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for encoding_dim in encoding_dims:\n",
    "            encoder_layers.extend([\n",
    "                nn.Linear(prev_dim, encoding_dim),\n",
    "                nn.BatchNorm1d(encoding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = encoding_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        decoding_dims = list(reversed(encoding_dims[:-1])) + [input_dim]\n",
    "        \n",
    "        for i, decoding_dim in enumerate(decoding_dims):\n",
    "            if i == len(decoding_dims) - 1:  # Last layer\n",
    "                decoder_layers.append(nn.Linear(prev_dim, decoding_dim))\n",
    "            else:\n",
    "                decoder_layers.extend([\n",
    "                    nn.Linear(prev_dim, decoding_dim),\n",
    "                    nn.BatchNorm1d(decoding_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout_rate)\n",
    "                ])\n",
    "            prev_dim = decoding_dim\n",
    "        \n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "    \n",
    "    def get_reconstruction_error(self, x):\n",
    "        with torch.no_grad():\n",
    "            decoded, _ = self.forward(x)\n",
    "            mse = torch.mean((x - decoded) ** 2, dim=1)\n",
    "            return mse.cpu().numpy()\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Comprehensive model trainer for eDNA classification\"\"\"\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.encoders = {}\n",
    "        self.training_history = {}\n",
    "        \n",
    "    def prepare_data(self, X, y, test_size=0.2, val_size=0.2):\n",
    "        \"\"\"Prepare train/validation/test splits\"\"\"\n",
    "        # First split: separate test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Second split: separate train and validation\n",
    "        val_size_adjusted = val_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def scale_features(self, X_train, X_val, X_test, scaler_name):\n",
    "        \"\"\"Scale features and store scaler\"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        self.scalers[scaler_name] = scaler\n",
    "        \n",
    "        return X_train_scaled, X_val_scaled, X_test_scaled\n",
    "    \n",
    "    def train_classifier(self, X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                        model_name, num_epochs=100, lr=0.001, batch_size=32):\n",
    "        \"\"\"Train taxonomic classifier\"\"\"\n",
    "        print(f\"Training {model_name} classifier...\")\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = self.scale_features(\n",
    "            X_train, X_val, X_test, f\"{model_name}_scaler\"\n",
    "        )\n",
    "        \n",
    "        # Prepare data loaders\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_train_scaled), \n",
    "            torch.LongTensor(y_train)\n",
    "        )\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.FloatTensor(X_val_scaled), \n",
    "            torch.LongTensor(y_val)\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        num_classes = len(np.unique(np.concatenate([y_train, y_val, y_test])))\n",
    "        model = TaxonomicClassifier(\n",
    "            input_dim=X_train_scaled.shape[1],\n",
    "            num_classes=num_classes\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        best_val_acc = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += batch_y.size(0)\n",
    "                    correct += (predicted == batch_y).sum().item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            val_acc = correct / total\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                torch.save(model.state_dict(), f\"{model_name}_best.pth\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 20:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_best.pth\"))\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_dataset = TensorDataset(torch.FloatTensor(X_test_scaled), torch.LongTensor(y_test))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "        \n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                batch_X = batch_X.to(self.device)\n",
    "                outputs = model(batch_X)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "                y_true.extend(batch_y.numpy())\n",
    "        \n",
    "        test_acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        # Store model and results\n",
    "        self.models[model_name] = model\n",
    "        self.training_history[model_name] = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'val_accuracies': val_accuracies,\n",
    "            'test_accuracy': test_acc,\n",
    "            'y_true': y_true,\n",
    "            'y_pred': y_pred\n",
    "        }\n",
    "        \n",
    "        return model, test_acc\n",
    "    \n",
    "    def train_novelty_detector(self, X_train, X_val, X_test, model_name, \n",
    "                              num_epochs=100, lr=0.001, batch_size=32):\n",
    "        \"\"\"Train autoencoder for novelty detection\"\"\"\n",
    "        print(f\"Training {model_name} novelty detector...\")\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled, X_val_scaled, X_test_scaled = self.scale_features(\n",
    "            X_train, X_val, X_test, f\"{model_name}_scaler\"\n",
    "        )\n",
    "        \n",
    "        # Prepare data loaders (autoencoder uses input as target)\n",
    "        train_dataset = TensorDataset(torch.FloatTensor(X_train_scaled), torch.FloatTensor(X_train_scaled))\n",
    "        val_dataset = TensorDataset(torch.FloatTensor(X_val_scaled), torch.FloatTensor(X_val_scaled))\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = NoveltyDetector(input_dim=X_train_scaled.shape[1]).to(self.device)\n",
    "        \n",
    "        # Loss and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                decoded, _ = model(batch_X)\n",
    "                loss = criterion(decoded, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss += loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in val_loader:\n",
    "                    batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                    decoded, _ = model(batch_X)\n",
    "                    loss = criterion(decoded, batch_y)\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            val_loss /= len(val_loader)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"{model_name}_best.pth\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= 20:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        model.load_state_dict(torch.load(f\"{model_name}_best.pth\"))\n",
    "        \n",
    "        # Store model and results\n",
    "        self.models[model_name] = model\n",
    "        self.training_history[model_name] = {\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(device=device)\n",
    "print(\"Model trainer initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19918f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Models ===\n",
      "\n",
      "--- Training Database Classifier ---\n",
      "Training database_classifier classifier...\n",
      "Epoch 0: Train Loss: 1.0778, Val Loss: 1.0293, Val Acc: 0.4083\n",
      "Epoch 0: Train Loss: 1.0778, Val Loss: 1.0293, Val Acc: 0.4083\n",
      "Epoch 10: Train Loss: 0.9663, Val Loss: 1.0253, Val Acc: 0.4030\n",
      "Epoch 10: Train Loss: 0.9663, Val Loss: 1.0253, Val Acc: 0.4030\n",
      "Epoch 20: Train Loss: 0.8959, Val Loss: 1.0473, Val Acc: 0.4110\n",
      "Epoch 20: Train Loss: 0.8959, Val Loss: 1.0473, Val Acc: 0.4110\n",
      "Epoch 30: Train Loss: 0.8546, Val Loss: 1.0703, Val Acc: 0.4003\n",
      "Epoch 30: Train Loss: 0.8546, Val Loss: 1.0703, Val Acc: 0.4003\n",
      "Epoch 40: Train Loss: 0.8273, Val Loss: 1.0944, Val Acc: 0.4047\n",
      "Epoch 40: Train Loss: 0.8273, Val Loss: 1.0944, Val Acc: 0.4047\n",
      "Early stopping at epoch 42\n",
      "Test Accuracy: 0.4153\n",
      "\n",
      "--- Training Cluster Classifier ---\n",
      "Not enough clusters for cluster classification\n",
      "\n",
      "--- Training Novelty Detector ---\n",
      "Training novelty_detector novelty detector...\n",
      "Early stopping at epoch 42\n",
      "Test Accuracy: 0.4153\n",
      "\n",
      "--- Training Cluster Classifier ---\n",
      "Not enough clusters for cluster classification\n",
      "\n",
      "--- Training Novelty Detector ---\n",
      "Training novelty_detector novelty detector...\n",
      "Epoch 0: Train Loss: 0.999087, Val Loss: 0.896209\n",
      "Epoch 0: Train Loss: 0.999087, Val Loss: 0.896209\n",
      "Epoch 10: Train Loss: 0.838161, Val Loss: 0.766570\n",
      "Epoch 10: Train Loss: 0.838161, Val Loss: 0.766570\n",
      "Epoch 20: Train Loss: 0.824138, Val Loss: 0.747606\n",
      "Epoch 20: Train Loss: 0.824138, Val Loss: 0.747606\n",
      "Epoch 30: Train Loss: 0.817310, Val Loss: 0.744085\n",
      "Epoch 30: Train Loss: 0.817310, Val Loss: 0.744085\n",
      "Epoch 40: Train Loss: 0.811479, Val Loss: 0.742095\n",
      "Epoch 40: Train Loss: 0.811479, Val Loss: 0.742095\n",
      "Epoch 50: Train Loss: 0.805023, Val Loss: 0.742822\n",
      "Epoch 50: Train Loss: 0.805023, Val Loss: 0.742822\n",
      "Epoch 60: Train Loss: 0.803739, Val Loss: 0.739247\n",
      "Epoch 60: Train Loss: 0.803739, Val Loss: 0.739247\n",
      "Epoch 70: Train Loss: 0.802306, Val Loss: 0.741231\n",
      "Epoch 70: Train Loss: 0.802306, Val Loss: 0.741231\n",
      "Epoch 80: Train Loss: 0.800511, Val Loss: 0.733593\n",
      "Epoch 80: Train Loss: 0.800511, Val Loss: 0.733593\n",
      "Epoch 90: Train Loss: 0.798993, Val Loss: 0.735804\n",
      "Epoch 90: Train Loss: 0.798993, Val Loss: 0.735804\n",
      "\n",
      "=== Model Training Completed ===\n",
      "Database classifier accuracy: 0.4153\n",
      "Novelty detector trained successfully\n",
      "\n",
      "=== Model Training Completed ===\n",
      "Database classifier accuracy: 0.4153\n",
      "Novelty detector trained successfully\n"
     ]
    }
   ],
   "source": [
    "# Train models for different tasks\n",
    "print(\"=== Training Models ===\")\n",
    "\n",
    "# 1. Database Classification Model\n",
    "print(\"\\n--- Training Database Classifier ---\")\n",
    "X_train_db, X_val_db, X_test_db, y_train_db, y_val_db, y_test_db = trainer.prepare_data(\n",
    "    X_pca, y_database_encoded\n",
    ")\n",
    "\n",
    "db_model, db_accuracy = trainer.train_classifier(\n",
    "    X_train_db, X_val_db, X_test_db, y_train_db, y_val_db, y_test_db,\n",
    "    model_name=\"database_classifier\",\n",
    "    num_epochs=50,\n",
    "    lr=0.001\n",
    ")\n",
    "\n",
    "# 2. Cluster Prediction Model (only for non-noise sequences)\n",
    "print(\"\\n--- Training Cluster Classifier ---\")\n",
    "if len(np.unique(y_cluster_encoded)) > 1:  # Only if we have multiple clusters\n",
    "    X_train_cl, X_val_cl, X_test_cl, y_train_cl, y_val_cl, y_test_cl = trainer.prepare_data(\n",
    "        X_clustered, y_cluster_encoded\n",
    "    )\n",
    "    \n",
    "    cluster_model, cluster_accuracy = trainer.train_classifier(\n",
    "        X_train_cl, X_val_cl, X_test_cl, y_train_cl, y_val_cl, y_test_cl,\n",
    "        model_name=\"cluster_classifier\", \n",
    "        num_epochs=75,\n",
    "        lr=0.001\n",
    "    )\n",
    "else:\n",
    "    print(\"Not enough clusters for cluster classification\")\n",
    "\n",
    "# 3. Novelty Detection Model\n",
    "print(\"\\n--- Training Novelty Detector ---\")\n",
    "# For novelty detection, we train on known sequences only\n",
    "known_mask = y_novelty == 0\n",
    "X_known = X_pca[known_mask]\n",
    "\n",
    "if len(X_known) > 50:  # Only if we have enough known sequences\n",
    "    X_train_nov, X_val_nov, X_test_nov, _, _, _ = trainer.prepare_data(\n",
    "        X_known, np.zeros(len(X_known))  # Dummy targets for autoencoders\n",
    "    )\n",
    "    \n",
    "    novelty_model = trainer.train_novelty_detector(\n",
    "        X_train_nov, X_val_nov, X_test_nov,\n",
    "        model_name=\"novelty_detector\",\n",
    "        num_epochs=100,\n",
    "        lr=0.001\n",
    "    )\n",
    "else:\n",
    "    print(\"Not enough known sequences for novelty detection training\")\n",
    "\n",
    "print(\"\\n=== Model Training Completed ===\")\n",
    "print(f\"Database classifier accuracy: {db_accuracy:.4f}\")\n",
    "if 'cluster_classifier' in trainer.training_history:\n",
    "    print(f\"Cluster classifier accuracy: {trainer.training_history['cluster_classifier']['test_accuracy']:.4f}\")\n",
    "if 'novelty_detector' in trainer.training_history:\n",
    "    print(f\"Novelty detector trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6524c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Saving Models and Results ===\n",
      "Saved database_classifier to ../model/dnabert_finetuned/database_classifier.pth\n",
      "Saved novelty_detector to ../model/dnabert_finetuned/novelty_detector.pth\n",
      "Saved preprocessing data to ../model/dnabert_finetuned/preprocessing_data.pkl\n",
      "Saved training results to ../model/dnabert_finetuned/training_results.pkl\n",
      "Saved model configuration to ../model/dnabert_finetuned/model_config.json\n",
      "\n",
      "=== Model Training Complete ===\n",
      "Models saved to: ../model/dnabert_finetuned\n",
      "Available models: ['database_classifier', 'novelty_detector']\n",
      "\n",
      "Model files created:\n",
      "  - *.pth files (PyTorch model weights)\n",
      "  - preprocessing_data.pkl (scalers and encoders)\n",
      "  - training_results.pkl (training history and metrics)\n",
      "  - model_config.json (model configuration)\n",
      "\n",
      "Ready for model evaluation!\n"
     ]
    }
   ],
   "source": [
    "# Save trained models and encoders\n",
    "print(\"\\n=== Saving Models and Results ===\")\n",
    "\n",
    "# Create model save directory\n",
    "model_save_dir = DNABERT_DIR\n",
    "model_save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save PyTorch models\n",
    "for model_name, model in trainer.models.items():\n",
    "    model_path = model_save_dir / f\"{model_name}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Saved {model_name} to {model_path}\")\n",
    "\n",
    "# Save scalers and encoders\n",
    "preprocessing_data = {\n",
    "    'scalers': trainer.scalers,\n",
    "    'database_encoder': database_encoder,\n",
    "    'cluster_encoder': cluster_encoder if 'cluster_classifier' in trainer.models else None,\n",
    "    'embedding_metadata': {\n",
    "        'original_dim': X_original.shape[1],\n",
    "        'pca_dim': X_pca.shape[1],\n",
    "        'num_sequences': len(df_sequences)\n",
    "    }\n",
    "}\n",
    "\n",
    "preprocessing_path = model_save_dir / \"preprocessing_data.pkl\"\n",
    "with open(preprocessing_path, 'wb') as f:\n",
    "    pickle.dump(preprocessing_data, f)\n",
    "print(f\"Saved preprocessing data to {preprocessing_path}\")\n",
    "\n",
    "# Save training history and results\n",
    "results_data = {\n",
    "    'training_history': trainer.training_history,\n",
    "    'model_architectures': {\n",
    "        'database_classifier': {\n",
    "            'input_dim': X_pca.shape[1],\n",
    "            'num_classes': len(np.unique(y_database_encoded)),\n",
    "            'task': 'database_classification'\n",
    "        },\n",
    "        'cluster_classifier': {\n",
    "            'input_dim': X_clustered.shape[1] if len(X_clustered) > 0 else X_pca.shape[1],\n",
    "            'num_classes': len(np.unique(y_cluster_encoded)) if 'cluster_classifier' in trainer.models else 0,\n",
    "            'task': 'cluster_classification'\n",
    "        } if 'cluster_classifier' in trainer.models else None,\n",
    "        'novelty_detector': {\n",
    "            'input_dim': X_pca.shape[1],\n",
    "            'task': 'novelty_detection'\n",
    "        } if 'novelty_detector' in trainer.models else None\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'database_accuracy': db_accuracy,\n",
    "        'cluster_accuracy': trainer.training_history.get('cluster_classifier', {}).get('test_accuracy', 0),\n",
    "    }\n",
    "}\n",
    "\n",
    "results_path = model_save_dir / \"training_results.pkl\"\n",
    "with open(results_path, 'wb') as f:\n",
    "    pickle.dump(results_data, f)\n",
    "print(f\"Saved training results to {results_path}\")\n",
    "\n",
    "# Create a simple model configuration file\n",
    "config = {\n",
    "    'model_version': '1.0',\n",
    "    'training_date': str(pd.Timestamp.now()),\n",
    "    'models_available': list(trainer.models.keys()),\n",
    "    'database_classes': database_encoder.classes_.tolist(),\n",
    "    'cluster_classes': cluster_encoder.classes_.tolist() if 'cluster_classifier' in trainer.models else [],\n",
    "    'embedding_dim': X_pca.shape[1],\n",
    "    'device_used': str(device),\n",
    "    'performance': {\n",
    "        'database_classification_accuracy': float(db_accuracy),\n",
    "        'cluster_classification_accuracy': float(trainer.training_history.get('cluster_classifier', {}).get('test_accuracy', 0))\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = model_save_dir / \"model_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Saved model configuration to {config_path}\")\n",
    "\n",
    "print(f\"\\n=== Model Training Complete ===\")\n",
    "print(f\"Models saved to: {model_save_dir}\")\n",
    "print(f\"Available models: {list(trainer.models.keys())}\")\n",
    "print(\"\\nModel files created:\")\n",
    "print(\"  - *.pth files (PyTorch model weights)\")\n",
    "print(\"  - preprocessing_data.pkl (scalers and encoders)\")\n",
    "print(\"  - training_results.pkl (training history and metrics)\")\n",
    "print(\"  - model_config.json (model configuration)\")\n",
    "print(\"\\nReady for model evaluation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edna-ml-backend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
